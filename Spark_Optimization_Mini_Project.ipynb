{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark-Optimization-Mini-Project",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPtgYE/m85OVA5vcrFXjwqb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaddanki001/Spark-Optimization-Mini-Project/blob/master/Spark_Optimization_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e2vnI1XF49i"
      },
      "source": [
        "### **Hadoop and Spark Installation Part**\r\n",
        "\r\n",
        "Hadoop is a Java-based programming framework that supports the processing and storage of extremely large datasets on a cluster of inexpensive machines. It was the first major open source project in the big data playing field and is sponsored by the Apache Software Foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUoudk59F6Xr"
      },
      "source": [
        "### **Pre Installation Steps **\r\n",
        "1. Clean up old files\r\n",
        "2. GIT Clone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHUOKIrMF-WL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5ee480-d46b-475e-e613-c0b8d300cf13"
      },
      "source": [
        "!rm -R Spark-Optimization-Mini-Project\r\n",
        "!rm spark-3.0.2-bin-hadoop2.7-hive1.2.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Spark-Optimization-Mini-Project': No such file or directory\n",
            "rm: cannot remove 'spark-3.0.2-bin-hadoop2.7-hive1.2.tgz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6yz-dpdGFpb",
        "outputId": "29232c1d-be80-4211-90e7-666a7fac5583"
      },
      "source": [
        "!git clone https://github.com/vaddanki001/Spark-Optimization-Mini-Project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Spark-Optimization-Mini-Project'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 21 (delta 1), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYcL5rcxF9D8"
      },
      "source": [
        "### **Step 1:Installing Hadoop and Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8PagvyWGZap"
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7-hive1.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnGWGo3BGfYR",
        "outputId": "a4fdb5e8-bdff-42a9-e6af-9c39851c7c55"
      },
      "source": [
        "!tar xf spark-3.0.2-bin-hadoop2.7-hive1.2.tgz\r\n",
        "\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 74kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 17.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=e8e5293c2026e75cf93a636e4b962483f8e2cfce5289a6ec32d91f1b3ee8838a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1rvXmE1GpZl"
      },
      "source": [
        "### **Step2: install JDK**\r\n",
        "Hadoop/Spark requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdzuWzT7GqAX",
        "outputId": "405924be-33a9-4c40-d6aa-ba236ca6d89d"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "#To find the default Java path\r\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWSUzB8PGty2"
      },
      "source": [
        "### Step 3: Setting **Java and Spark Home**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnRSyDaiGwaI"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7-hive1.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6maKYHd1GzmK"
      },
      "source": [
        "### Step 4: **Initiate Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaHbpKr8G1tc"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n",
        "\r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01JssMSlG79P"
      },
      "source": [
        "### Step 5: **Check Datafile exists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbNt50dG_JG",
        "outputId": "fcedb83a-7baf-40d7-8879-b6cae444ea6e"
      },
      "source": [
        "!ls -ltr /content/Spark-Optimization-Mini-Project/data/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar  1 21:24 answers\n",
            "drwxr-xr-x 2 root root 4096 Mar  1 21:24 questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgHKZmFDH1cE"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\r\n",
        "import pyspark\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import col, count, month\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "# create the session\r\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\r\n",
        "\r\n",
        "# create the context\r\n",
        "sc = pyspark.SparkContext(conf=conf)\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Optimize I').getOrCreate()\r\n",
        "# spark = SparkSession.builder.appName('Optimize I').getOrCreate()\r\n",
        "\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1N_G4DzRU7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a982b38b-7f54-4582-ee57-35a3cc2314be"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "def create_spark_ui_link():\r\n",
        "  !wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\r\n",
        "  !unzip -qq ngrok-stable-linux-amd64.zip\r\n",
        "  get_ipython().system_raw('./ngrok http 4050 &')\r\n",
        "  time.sleep(3) # For when the tunnel takes seconds to open\r\n",
        "  !curl -s http://localhost:4040/api/tunnels | python -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\r\n",
        "\r\n",
        "create_spark_ui_link()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "https://6f407bd6aff9.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_dYncyRSWs"
      },
      "source": [
        "base_path = os.getcwd()\r\n",
        "\r\n",
        "project_path = ('/').join(base_path.split('/')[0:-3]) \r\n",
        "\r\n",
        "answers_input_path = os.path.join(project_path, '/content/Spark-Optimization-Mini-Project/data/answers')\r\n",
        "\r\n",
        "questions_input_path = os.path.join(project_path, '/content/Spark-Optimization-Mini-Project/data/questions')\r\n",
        "\r\n",
        "answersDF = spark.read.option('path', answers_input_path).load()\r\n",
        "\r\n",
        "questionsDF = spark.read.option('path', questions_input_path).load()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jnORu7JeZH",
        "outputId": "d5dcdc4d-0554-45b9-e76c-7bd37c5b62f2"
      },
      "source": [
        "answersDF.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+--------------------+--------+-------+-----+\n",
            "|question_id|answer_id|       creation_date|comments|user_id|score|\n",
            "+-----------+---------+--------------------+--------+-------+-----+\n",
            "|     226592|   226595|2015-12-29 23:46:...|       3|  82798|    2|\n",
            "|     388057|   388062|2018-02-22 18:52:...|       8|    520|   21|\n",
            "|     293286|   293305|2016-11-17 21:35:...|       0|  47472|    2|\n",
            "|     442499|   442503|2018-11-22 06:34:...|       0| 137289|    0|\n",
            "|     293009|   293031|2016-11-16 13:36:...|       0|  83721|    0|\n",
            "+-----------+---------+--------------------+--------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ook5cTkcIbNI",
        "outputId": "6d09ee41-329c-4352-8a43-7ed916ef2faf"
      },
      "source": [
        "questionsDF.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
            "|question_id|                tags|       creation_date|               title|accepted_answer_id|comments|user_id|views|\n",
            "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
            "|     382738|[optics, waves, f...|2018-01-28 07:22:...|What is the pseud...|            382772|       0|  76347|   32|\n",
            "|     370717|[field-theory, de...|2017-11-25 09:09:...|What is the defin...|              null|       1|  75085|   82|\n",
            "|     339944|[general-relativi...|2017-06-17 20:32:...|Could gravitation...|              null|      13| 116137|  333|\n",
            "|     233852|[homework-and-exe...|2016-02-04 21:19:...|When does travell...|              null|       9|  95831|  185|\n",
            "|     294165|[quantum-mechanic...|2016-11-22 11:39:...|Time-dependent qu...|              null|       1| 118807|   56|\n",
            "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXn2uvnfJnwz",
        "outputId": "d90f70ba-976a-4562-b362-0199fbede8d3"
      },
      "source": [
        "answers_month = answersDF.withColumn('month', month('creation_date')).groupBy('question_id', 'month').agg(count('*').alias('cnt'))\r\n",
        "\r\n",
        "resultDF = questionsDF.join(answers_month, 'question_id').select('question_id', 'creation_date', 'title', 'month', 'cnt')\r\n",
        "\r\n",
        "resultDF.orderBy('question_id', 'month').show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+--------------------+-----+---+\n",
            "|question_id|       creation_date|               title|month|cnt|\n",
            "+-----------+--------------------+--------------------+-----+---+\n",
            "|     155989|2015-01-01 01:59:...|Frost bubble form...|    1|  1|\n",
            "|     155989|2015-01-01 01:59:...|Frost bubble form...|    2|  1|\n",
            "|     155990|2015-01-01 02:51:...|The abstract spac...|    1|  2|\n",
            "|     155992|2015-01-01 03:44:...|centrifugal force...|    1|  1|\n",
            "|     155993|2015-01-01 03:56:...|How can I estimat...|    1|  1|\n",
            "|     155995|2015-01-01 05:16:...|Why should a solu...|    1|  3|\n",
            "|     155996|2015-01-01 06:06:...|Why do we assume ...|    1|  2|\n",
            "|     155996|2015-01-01 06:06:...|Why do we assume ...|    2|  1|\n",
            "|     155996|2015-01-01 06:06:...|Why do we assume ...|   11|  1|\n",
            "|     155997|2015-01-01 06:26:...|Why do square sha...|    1|  3|\n",
            "|     155999|2015-01-01 07:01:...|Diagonalizability...|    1|  1|\n",
            "|     156008|2015-01-01 08:48:...|Capturing a light...|    1|  2|\n",
            "|     156008|2015-01-01 08:48:...|Capturing a light...|   11|  1|\n",
            "|     156016|2015-01-01 10:31:...|The interference ...|    1|  1|\n",
            "|     156020|2015-01-01 11:19:...|What is going on ...|    1|  1|\n",
            "|     156021|2015-01-01 11:21:...|How to calculate ...|    2|  1|\n",
            "|     156022|2015-01-01 11:55:...|Advice on Major S...|    1|  1|\n",
            "|     156025|2015-01-01 12:32:...|Deriving the Cano...|    1|  1|\n",
            "|     156026|2015-01-01 12:49:...|Does Bell's inequ...|    1|  3|\n",
            "|     156027|2015-01-01 12:49:...|Deriving X atom f...|    1|  1|\n",
            "+-----------+--------------------+--------------------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI9HuwKOJpes",
        "outputId": "bac2c1dd-3783-4a1b-942c-8d9ecb8b2bb4"
      },
      "source": [
        "resultDF.orderBy('question_id', 'month').explain()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(4) Sort [question_id#329L ASC NULLS FIRST, month#345 ASC NULLS FIRST], true, 0\n",
            "+- Exchange rangepartitioning(question_id#329L ASC NULLS FIRST, month#345 ASC NULLS FIRST, 200), true, [id=#766]\n",
            "   +- *(3) Project [question_id#329L, creation_date#331, title#332, month#345, cnt#361L]\n",
            "      +- *(3) BroadcastHashJoin [question_id#329L], [question_id#317L], Inner, BuildRight\n",
            "         :- *(3) Project [question_id#329L, creation_date#331, title#332]\n",
            "         :  +- *(3) Filter isnotnull(question_id#329L)\n",
            "         :     +- *(3) ColumnarToRow\n",
            "         :        +- FileScan parquet [question_id#329L,creation_date#331,title#332] Batched: true, DataFilters: [isnotnull(question_id#329L)], Format: Parquet, Location: InMemoryFileIndex[file:/content/Spark-Optimization-Mini-Project/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
            "         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true])), [id=#761]\n",
            "            +- *(2) HashAggregate(keys=[question_id#317L, month#345], functions=[count(1)])\n",
            "               +- Exchange hashpartitioning(question_id#317L, month#345, 200), true, [id=#757]\n",
            "                  +- *(1) HashAggregate(keys=[question_id#317L, month#345], functions=[partial_count(1)])\n",
            "                     +- *(1) Project [question_id#317L, month(cast(creation_date#319 as date)) AS month#345]\n",
            "                        +- *(1) Filter isnotnull(question_id#317L)\n",
            "                           +- *(1) ColumnarToRow\n",
            "                              +- FileScan parquet [question_id#317L,creation_date#319] Batched: true, DataFilters: [isnotnull(question_id#317L)], Format: Parquet, Location: InMemoryFileIndex[file:/content/Spark-Optimization-Mini-Project/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDj3VOR6MHVm",
        "outputId": "8c94012f-04e7-4d34-ee37-d097225f79af"
      },
      "source": [
        "questionsDF.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et9mxGTOmfpn",
        "outputId": "056c51e5-6986-47d3-f4be-e9f64f8a0333"
      },
      "source": [
        "answersDF.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5maKlvhyoI-R",
        "outputId": "f67b2e9a-876a-4be8-d4b7-a807c672852f"
      },
      "source": [
        "answers_month.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----+---+\n",
            "|question_id|month|cnt|\n",
            "+-----------+-----+---+\n",
            "|     358894|    9|  5|\n",
            "|     332782|    5|  2|\n",
            "|     281552|    9|  2|\n",
            "|     332224|    5|  1|\n",
            "|     395851|    3|  3|\n",
            "+-----------+-----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgBfKp5oDVb"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOuFJrjd-2pa"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import col, count, month\r\n",
        "import findspark\r\n",
        "import os\r\n",
        "\r\n",
        "\r\n",
        "findspark.init()\r\n",
        "\r\n",
        "# create the session\r\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\").set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\").set(\"spark.executor.fraction\",\"0.9\")\r\n",
        "\r\n",
        "# create the context\r\n",
        "sc = pyspark.SparkContext(conf=conf)\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n",
        "# spark = SparkSession.builder.appName('Optimize I').getOrCreate()\r\n",
        "# spark.conf.set(\"spark.executor.memory\", \"4g\")\r\n",
        "# spark.conf.set(\"spark.driver.memory\", \"4g\")\r\n",
        "# spark.conf.set(\"spark.executor.fraction\",\"0.9\")\r\n",
        "sc.setSystemProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E2GB8Z32C5n"
      },
      "source": [
        "base_path = os.getcwd()\r\n",
        "\r\n",
        "project_path = ('/').join(base_path.split('/')[0:-3]) \r\n",
        "\r\n",
        "answers_input_path = os.path.join(project_path, '/content/Spark-Optimization-Mini-Project/data/answers')\r\n",
        "\r\n",
        "questions_input_path = os.path.join(project_path, '/content/Spark-Optimization-Mini-Project/data/questions')\r\n",
        "\r\n",
        "answersDF = spark.read.option('path', answers_input_path).load()\r\n",
        "\r\n",
        "questionsDF = spark.read.option('path', questions_input_path).load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "498LJzmkmi2Y",
        "outputId": "f6fc563e-63d1-4bdf-8325-10fd2af5f276"
      },
      "source": [
        "answersDF.registerTempTable(\"answersDFTbl\")\r\n",
        "questionsDF.registerTempTable(\"questionsDFTbl\")\r\n",
        "answersDF.coalesce(1)\r\n",
        "answers_monthsDF = spark.sql(\"select question_id, month(creation_date) as month, count(*) as cnt from answersDFTbl group by question_id, month\")\r\n",
        "answers_monthsDF.registerTempTable(\"answers_monthsDFTbl\")\r\n",
        "\r\n",
        "# answers_monthsTbl.show(5)\r\n",
        "# spark.sql(\"select * from answersDFtbl\").show(5)\r\n",
        "\r\n",
        "\r\n",
        "resultDFTbl = spark.sql(\"SELECT /*+ BROADCAST (answers_monthsDFTbl) */ q.question_id, q.creation_date, title,am.month, am.cnt FROM questionsDFTbl q , answers_monthsDFTbl am  where  q.question_id = am.question_id order by q.question_id, am.month\")\r\n",
        "# resultDFTbl = spark.sql(\"SELECT /*+ BROADCAST (answersDFTbl) */ q.question_id, q.creation_date, title, month(am.creation_date) as month, COUNT(*) OVER (PARTITION BY q.question_id) AS cnt FROM questionsDFTbl q , answersDFTbl am  where  q.question_id = am.question_id \")\r\n",
        "\r\n",
        "resultDFTbl.explain()\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(4) Sort [question_id#187L ASC NULLS FIRST, month#217 ASC NULLS FIRST], true, 0\n",
            "+- Exchange rangepartitioning(question_id#187L ASC NULLS FIRST, month#217 ASC NULLS FIRST, 200), true, [id=#380]\n",
            "   +- *(3) Project [question_id#187L, creation_date#189, title#190, month#217, cnt#218L]\n",
            "      +- *(3) BroadcastHashJoin [question_id#187L], [question_id#175L], Inner, BuildRight\n",
            "         :- *(3) Project [question_id#187L, creation_date#189, title#190]\n",
            "         :  +- *(3) Filter isnotnull(question_id#187L)\n",
            "         :     +- *(3) ColumnarToRow\n",
            "         :        +- FileScan parquet [question_id#187L,creation_date#189,title#190] Batched: true, DataFilters: [isnotnull(question_id#187L)], Format: Parquet, Location: InMemoryFileIndex[file:/content/Spark-Optimization-Mini-Project/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
            "         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true])), [id=#375]\n",
            "            +- *(2) HashAggregate(keys=[question_id#175L, month(cast(creation_date#177 as date))#228], functions=[count(1)])\n",
            "               +- Exchange hashpartitioning(question_id#175L, month(cast(creation_date#177 as date))#228, 200), true, [id=#371]\n",
            "                  +- *(1) HashAggregate(keys=[question_id#175L, month(cast(creation_date#177 as date)) AS month(cast(creation_date#177 as date))#228], functions=[partial_count(1)])\n",
            "                     +- *(1) Project [question_id#175L, creation_date#177]\n",
            "                        +- *(1) Filter isnotnull(question_id#175L)\n",
            "                           +- *(1) ColumnarToRow\n",
            "                              +- FileScan parquet [question_id#175L,creation_date#177] Batched: true, DataFilters: [isnotnull(question_id#175L)], Format: Parquet, Location: InMemoryFileIndex[file:/content/Spark-Optimization-Mini-Project/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV_frX1enEYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "73YZIM4vlj7R",
        "outputId": "79c024d2-6563-455a-e672-cdfa76b583d3"
      },
      "source": [
        "spark"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7bd7523288d1:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe30801de10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGMsTqLn1WS"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}