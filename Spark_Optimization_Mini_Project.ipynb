{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark-Optimization-Mini-Project",
      "provenance": [],
      "authorship_tag": "ABX9TyMVbBoR1KZlqxdcy92O4Nfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaddanki001/Spark-Optimization-Mini-Project/blob/master/Spark_Optimization_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e2vnI1XF49i"
      },
      "source": [
        "### **Hadoop and Spark Installation Part**\r\n",
        "\r\n",
        "Hadoop is a Java-based programming framework that supports the processing and storage of extremely large datasets on a cluster of inexpensive machines. It was the first major open source project in the big data playing field and is sponsored by the Apache Software Foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUoudk59F6Xr"
      },
      "source": [
        "### **Pre Installation Steps **\r\n",
        "1. Clean up old files\r\n",
        "2. GIT Clone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHUOKIrMF-WL",
        "outputId": "f8672f80-dd56-4e37-e083-152b2aa3f9e9"
      },
      "source": [
        "!rm -R Spark-Optimization-Mini-Project\r\n",
        "!rm spark-3.0.2-bin-hadoop2.7-hive1.2.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Spark-Mini-Project': No such file or directory\n",
            "rm: cannot remove 'spark-3.0.2-bin-hadoop2.7-hive1.2.tgz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6yz-dpdGFpb",
        "outputId": "4360e5e4-93dc-441b-9760-bab761465c25"
      },
      "source": [
        "!git clone https://github.com/vaddanki001/Spark-Optimization-Mini-Project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Spark-Optimization-Mini-Project'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 15 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYcL5rcxF9D8"
      },
      "source": [
        "### **Step 1:Installing Hadoop and Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8PagvyWGZap"
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7-hive1.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnGWGo3BGfYR",
        "outputId": "50dbe811-efba-4e41-fef2-23105f9a4b33"
      },
      "source": [
        "!tar xf spark-3.0.2-bin-hadoop2.7-hive1.2.tgz\r\n",
        "\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 75kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 17.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=2593996efcd463d43b36db4145fb3fc462e1fdf24b156ecb8676659fff4d1bb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1rvXmE1GpZl"
      },
      "source": [
        "### **Step2: install JDK**\r\n",
        "Hadoop/Spark requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdzuWzT7GqAX",
        "outputId": "c112a3ef-d769-4ed1-cf58-bb79271bba93"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "#To find the default Java path\r\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWSUzB8PGty2"
      },
      "source": [
        "### Step 3: Setting **Java and Spark Home**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnRSyDaiGwaI"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7-hive1.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6maKYHd1GzmK"
      },
      "source": [
        "### Step 4: **Initiate Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaHbpKr8G1tc"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n",
        "\r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01JssMSlG79P"
      },
      "source": [
        "### Step 5: **Check Datafile exists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbNt50dG_JG",
        "outputId": "8ec036ff-8b24-4173-b09e-a0a570719983"
      },
      "source": [
        "!ls -ltr /content/Spark-Optimization-Mini-Project/data/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Feb 27 01:31 answers\n",
            "drwxr-xr-x 2 root root 4096 Feb 27 01:31 questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgHKZmFDH1cE"
      },
      "source": [
        "import pyspark\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import col, count, month\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName('Optimize I').getOrCreate()\r\n",
        "\r\n",
        "base_path = os.getcwd()\r\n",
        "\r\n",
        "project_path = ('/').join(base_path.split('/')[0:-3]) \r\n",
        "\r\n",
        "answers_input_path = os.path.join(project_path, '/content/Spark-Optimization-Mini-Project/data/answers')\r\n",
        "\r\n",
        "questions_input_path = os.path.join(project_path, '/content/Spark-Optimization-Mini-Project/data/questions')\r\n",
        "\r\n",
        "answersDF = spark.read.option('path', answers_input_path).load()\r\n",
        "\r\n",
        "questionsDF = spark.read.option('path', questions_input_path).load()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jnORu7JeZH",
        "outputId": "aec7664a-f378-4251-a0e5-61b97408509f"
      },
      "source": [
        "answersDF.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+--------------------+--------+-------+-----+\n",
            "|question_id|answer_id|       creation_date|comments|user_id|score|\n",
            "+-----------+---------+--------------------+--------+-------+-----+\n",
            "|     226592|   226595|2015-12-29 23:46:...|       3|  82798|    2|\n",
            "|     388057|   388062|2018-02-22 18:52:...|       8|    520|   21|\n",
            "|     293286|   293305|2016-11-17 21:35:...|       0|  47472|    2|\n",
            "|     442499|   442503|2018-11-22 06:34:...|       0| 137289|    0|\n",
            "|     293009|   293031|2016-11-16 13:36:...|       0|  83721|    0|\n",
            "+-----------+---------+--------------------+--------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ook5cTkcIbNI",
        "outputId": "eb905bdf-2078-42c6-d887-f9f7b564a777"
      },
      "source": [
        "questionsDF.show(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
            "|question_id|                tags|       creation_date|               title|accepted_answer_id|comments|user_id|views|\n",
            "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
            "|     382738|[optics, waves, f...|2018-01-28 07:22:...|What is the pseud...|            382772|       0|  76347|   32|\n",
            "|     370717|[field-theory, de...|2017-11-25 09:09:...|What is the defin...|              null|       1|  75085|   82|\n",
            "|     339944|[general-relativi...|2017-06-17 20:32:...|Could gravitation...|              null|      13| 116137|  333|\n",
            "|     233852|[homework-and-exe...|2016-02-04 21:19:...|When does travell...|              null|       9|  95831|  185|\n",
            "|     294165|[quantum-mechanic...|2016-11-22 11:39:...|Time-dependent qu...|              null|       1| 118807|   56|\n",
            "+-----------+--------------------+--------------------+--------------------+------------------+--------+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXn2uvnfJnwz",
        "outputId": "9cadeda5-3ff3-4b04-ebb4-171d4a107812"
      },
      "source": [
        "answers_month = answersDF.withColumn('month', month('creation_date')).groupBy('question_id', 'month').agg(count('*').alias('cnt'))\r\n",
        "\r\n",
        "resultDF = questionsDF.join(answers_month, 'question_id').select('question_id', 'creation_date', 'title', 'month', 'cnt')\r\n",
        "\r\n",
        "resultDF.orderBy('question_id', 'month').show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+--------------------+-----+---+\n",
            "|question_id|       creation_date|               title|month|cnt|\n",
            "+-----------+--------------------+--------------------+-----+---+\n",
            "|     155989|2015-01-01 01:59:...|Frost bubble form...|    1|  1|\n",
            "|     155989|2015-01-01 01:59:...|Frost bubble form...|    2|  1|\n",
            "|     155990|2015-01-01 02:51:...|The abstract spac...|    1|  2|\n",
            "|     155992|2015-01-01 03:44:...|centrifugal force...|    1|  1|\n",
            "|     155993|2015-01-01 03:56:...|How can I estimat...|    1|  1|\n",
            "|     155995|2015-01-01 05:16:...|Why should a solu...|    1|  3|\n",
            "|     155996|2015-01-01 06:06:...|Why do we assume ...|    1|  2|\n",
            "|     155996|2015-01-01 06:06:...|Why do we assume ...|    2|  1|\n",
            "|     155996|2015-01-01 06:06:...|Why do we assume ...|   11|  1|\n",
            "|     155997|2015-01-01 06:26:...|Why do square sha...|    1|  3|\n",
            "|     155999|2015-01-01 07:01:...|Diagonalizability...|    1|  1|\n",
            "|     156008|2015-01-01 08:48:...|Capturing a light...|    1|  2|\n",
            "|     156008|2015-01-01 08:48:...|Capturing a light...|   11|  1|\n",
            "|     156016|2015-01-01 10:31:...|The interference ...|    1|  1|\n",
            "|     156020|2015-01-01 11:19:...|What is going on ...|    1|  1|\n",
            "|     156021|2015-01-01 11:21:...|How to calculate ...|    2|  1|\n",
            "|     156022|2015-01-01 11:55:...|Advice on Major S...|    1|  1|\n",
            "|     156025|2015-01-01 12:32:...|Deriving the Cano...|    1|  1|\n",
            "|     156026|2015-01-01 12:49:...|Does Bell's inequ...|    1|  3|\n",
            "|     156027|2015-01-01 12:49:...|Deriving X atom f...|    1|  1|\n",
            "+-----------+--------------------+--------------------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI9HuwKOJpes",
        "outputId": "a5ff7971-95ea-458f-b49e-35cc566ea766"
      },
      "source": [
        "resultDF.explain()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(3) Project [question_id#12L, creation_date#14, title#15, month#154, cnt#170L]\n",
            "+- *(3) BroadcastHashJoin [question_id#12L], [question_id#0L], Inner, BuildRight\n",
            "   :- *(3) Project [question_id#12L, creation_date#14, title#15]\n",
            "   :  +- *(3) Filter isnotnull(question_id#12L)\n",
            "   :     +- *(3) ColumnarToRow\n",
            "   :        +- FileScan parquet [question_id#12L,creation_date#14,title#15] Batched: true, DataFilters: [isnotnull(question_id#12L)], Format: Parquet, Location: InMemoryFileIndex[file:/content/Spark-Optimization-Mini-Project/data/questions], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp,title:string>\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true])), [id=#213]\n",
            "      +- *(2) HashAggregate(keys=[question_id#0L, month#154], functions=[count(1)])\n",
            "         +- Exchange hashpartitioning(question_id#0L, month#154, 200), true, [id=#209]\n",
            "            +- *(1) HashAggregate(keys=[question_id#0L, month#154], functions=[partial_count(1)])\n",
            "               +- *(1) Project [question_id#0L, month(cast(creation_date#2 as date)) AS month#154]\n",
            "                  +- *(1) Filter isnotnull(question_id#0L)\n",
            "                     +- *(1) ColumnarToRow\n",
            "                        +- FileScan parquet [question_id#0L,creation_date#2] Batched: true, DataFilters: [isnotnull(question_id#0L)], Format: Parquet, Location: InMemoryFileIndex[file:/content/Spark-Optimization-Mini-Project/data/answers], PartitionFilters: [], PushedFilters: [IsNotNull(question_id)], ReadSchema: struct<question_id:bigint,creation_date:timestamp>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDj3VOR6MHVm",
        "outputId": "e3c67913-1a32-4a7d-b39d-75caa50eddc6"
      },
      "source": [
        "questionsDF.count()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}